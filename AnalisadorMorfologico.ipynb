{"cells":[{"cell_type":"markdown","metadata":{"id":"nnYfYa36GoCr"},"source":["# Atividade:Analisador Morfológico\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sbr2TfEOharC"},"source":["\n","\n","\n","1.Pré-processamento de dados: rotular usando predefinições as palavras mais óbvias\n"," *   1.1 Definição de funções de pré-processamento\n"," *   1.2 Extrair,tratar e préprocessar conjunto de dados\n"," *   1.3 Calcular a acurácia do pré-processamento e decidir se é viável utilizá-lo\n","\n","2.Machine Learning:\n"," *   2.1 Seleção de features (entrada) e target (saída)\n"," *  2.2 Seleção do modelo: utilizei árvores de decisão, pois apresentam baixo custo computacional e grande eficiência para este caso, uma vez que se baseiam em decisões, o que possui semelhanças com a forma como a língua portuguesa lida com a gramática.Utilizei grid search para o pos-tagging para descobrir os melhores parâmetros,mas,após achar estes parâmetros retirei o gridsearch devido o alto custo computacional para uma operação que pode ser feita uma única vez.\n"," *  2.3 Treinamento\n"," *  2.4 Acurácia do modelo\n","\n","*Para o algoritmo de flexões, foi necessário dividir o target (dicionário com vários atributos,logo se trata de um caso de classificação multipla) em colunas, aplicar one-hot encoding(valores categóricos que não seguem ordem,logo uma matriz espaçada aumentaria a eficiência do algoritmo) e realizar o treinamento para cada coluna.\n","\n","3.Criação da função Analisador Morfológico, utilizando tanto o pré-processamento quanto os modelos de machine learning\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bciKT6XKRKl_"},"source":["#1.Pré-processamento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tkuj9qzhdugb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fe17bbf-2505-4336-97b6-df31406bfd2c","executionInfo":{"status":"ok","timestamp":1724003505381,"user_tz":180,"elapsed":52059,"user":{"displayName":"Marlo Souza","userId":"17413101361011853171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting conllu\n","  Downloading conllu-5.0.1-py3-none-any.whl.metadata (21 kB)\n","Downloading conllu-5.0.1-py3-none-any.whl (16 kB)\n","Installing collected packages: conllu\n","Successfully installed conllu-5.0.1\n","--2024-08-18 17:51:04--  http://marlovss.work.gd:8080/tomorrow/aula2/pt_porttinari-ud-train.conllu\n","Resolving marlovss.work.gd (marlovss.work.gd)... 177.180.149.154\n","Connecting to marlovss.work.gd (marlovss.work.gd)|177.180.149.154|:8080... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7444078 (7.1M)\n","Saving to: ‘pt_porttinari-ud-train.conllu’\n","\n","pt_porttinari-ud-tr 100%[===================>]   7.10M   531KB/s    in 34s     \n","\n","2024-08-18 17:51:39 (211 KB/s) - ‘pt_porttinari-ud-train.conllu’ saved [7444078/7444078]\n","\n","--2024-08-18 17:51:39--  http://marlovss.work.gd:8080/tomorrow/aula2/pt_porttinari-ud-test.conllu\n","Resolving marlovss.work.gd (marlovss.work.gd)... 177.180.149.154\n","Connecting to marlovss.work.gd (marlovss.work.gd)|177.180.149.154|:8080... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2120477 (2.0M)\n","Saving to: ‘pt_porttinari-ud-test.conllu’\n","\n","pt_porttinari-ud-te 100%[===================>]   2.02M   523KB/s    in 4.5s    \n","\n","2024-08-18 17:51:44 (465 KB/s) - ‘pt_porttinari-ud-test.conllu’ saved [2120477/2120477]\n","\n"]}],"source":["!pip install conllu\n","!wget http://marlovss.work.gd:8080/tomorrow/aula2/pt_porttinari-ud-train.conllu\n","!wget http://marlovss.work.gd:8080/tomorrow/aula2/pt_porttinari-ud-test.conllu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gs2sKfI8O9ga"},"outputs":[],"source":["import conllu\n","import itertools as it\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{"id":"dhS27CZ4GoCy"},"source":["1.1 Definindo funções"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pThKGXkGoCz"},"outputs":[],"source":["class AttributeDict(dict):\n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__\n","\n","class CoNLLU:\n","    def __init__(self, files):\n","        self.words = []\n","        self.sentences = []\n","        for f in files:\n","            with open(f, encoding='utf-8') as file:\n","                parsed = conllu.parse(file.read())\n","            sents = [[AttributeDict(form=token['form'], lemma=token['lemma'], pos=token['upos'], feats=token['feats']) for token in tokenlist if token['upos'] != '_'] for tokenlist in parsed]\n","            self.sentences.extend(sents)\n","            self.words.extend([word for sent in sents for word in sent])\n","        self.pos_tags = set([word.pos for word in self.words])\n","        self.feats_dict = {pos: set(it.chain.from_iterable([list(word.feats.keys()) for word in self.words if word.pos == pos and word.feats is not None])) for pos in self.pos_tags}\n","def extract_data(conllu_data):\n","    texts = []\n","    pos_tags = []\n","    lemmas = []\n","    feats = []\n","    for word in conllu_data.words:\n","        texts.append(word.form)\n","        pos_tags.append(word.pos)\n","        lemmas.append(word.lemma)\n","        feats.append(word.feats if word.feats is not None else {})\n","    return texts, pos_tags, lemmas, feats\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkpyKG9vGoC0"},"outputs":[],"source":["\n","def preprocess_token(token):\n","    if re.match(r'^[\\.,;:!?\\'\\\"\\(\\)\\[\\]\\{\\}»«]', token):\n","        return ('PUNCT', token, token, {})\n","    if re.match(r'^[\\@\\#\\$\\%\\&\\*\\+\\=\\-\\_\\^\\~\\|\\\\\\/]+$', token):\n","        return ('SYM', token, token, {})\n","\n","    pronouns = {\n","        'eu': ('PRON', 'eu', 'eu', {'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'tu': ('PRON', 'tu', 'tu', {'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'ele': ('PRON', 'ele', 'ele', {'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'ela': ('PRON', 'ela', 'ela', {'Gender': 'Fem', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'nós': ('PRON', 'nós', 'nós', {'Gender': 'Masc', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'vós': ('PRON', 'vós', 'vós', {'Gender': 'Masc', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'eles': ('PRON', 'eles', 'eles', {'Gender': 'Masc', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'elas': ('PRON', 'elas', 'elas', {'Gender': 'Fem', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'meu': ('PRON', 'meu', 'meu', {'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'minha': ('PRON', 'minha', 'minha', {'Gender': 'Fem', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'teu': ('PRON', 'teu', 'teu', {'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'tua': ('PRON', 'tua', 'tua', {'Gender': 'Fem', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'seu': ('PRON', 'seu', 'seu', {'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'sua': ('PRON', 'sua', 'sua', {'Gender': 'Fem', 'Number': 'Sing', 'PronType': 'Prs'}),\n","        'nosso': ('PRON', 'nosso', 'nosso', {'Gender': 'Masc', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'nossa': ('PRON', 'nossa', 'nossa', {'Gender': 'Fem', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'vosso': ('PRON', 'vosso', 'vosso', {'Gender': 'Masc', 'Number': 'Plur', 'PronType': 'Prs'}),\n","        'vossa': ('PRON', 'vossa', 'vossa', {'Gender': 'Fem', 'Number': 'Plur', 'PronType': 'Prs'})\n","    }\n","    if token.lower() in pronouns:\n","        return pronouns[token.lower()]\n","\n","    if token.lower().endswith('mente'):\n","        return ('ADV', token, token, {})\n","\n","    prepositions = {'a', 'ante', 'após', 'até', 'com', 'contra', 'de', 'desde', 'em', 'entre', 'para', 'per', 'perante', 'por', 'sem', 'sob', 'sobre', 'trás'}\n","    if token.lower() in prepositions:\n","        return ('ADP', token, token, {})\n","\n","    determiners = {\n","        'o': ('DET', 'o', 'o', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'a': ('DET', 'a', 'a', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'os': ('DET', 'os', 'os', {'Gender': 'Masc', 'Number': 'Plur'}),\n","        'as': ('DET', 'as', 'as', {'Gender': 'Fem', 'Number': 'Plur'}),\n","        'um': ('DET', 'um', 'um', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'uma': ('DET', 'uma', 'uma', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'uns': ('DET', 'uns', 'uns', {'Gender': 'Masc', 'Number': 'Plur'}),\n","        'umas': ('DET', 'umas', 'umas', {'Gender': 'Fem', 'Number': 'Plur'}),\n","        'meu': ('DET', 'meu', 'meu', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'minha': ('DET', 'minha', 'minha', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'teu': ('DET', 'teu', 'teu', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'tua': ('DET', 'tua', 'tua', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'seu': ('DET', 'seu', 'seu', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'sua': ('DET', 'sua', 'sua', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'nosso': ('DET', 'nosso', 'nosso', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'nossa': ('DET', 'nossa', 'nossa', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'vosso': ('DET', 'vosso', 'vosso', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'vossa': ('DET', 'vossa', 'vossa', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'este': ('DET', 'este', 'este', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'esta': ('DET', 'esta', 'esta', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'esse': ('DET', 'esse', 'esse', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'essa': ('DET', 'essa', 'essa', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'aquele': ('DET', 'aquele', 'aquele', {'Gender': 'Masc', 'Number': 'Sing'}),\n","        'aquela': ('DET', 'aquela', 'aquela', {'Gender': 'Fem', 'Number': 'Sing'}),\n","        'isso': ('DET', 'isso', 'isso', {'Gender': 'Neut', 'Number': 'Sing'}),\n","        'isto': ('DET', 'isto', 'isto', {'Gender': 'Neut', 'Number': 'Sing'}),\n","        'aquilo': ('DET', 'aquilo', 'aquilo', {'Gender': 'Neut', 'Number': 'Sing'})\n","    }\n","    if token.lower() in determiners:\n","        return determiners[token.lower()]\n","\n","    cconj = {'e', 'ou', 'mas', 'porém', 'todavia', 'contudo', 'entretanto', 'nem', 'logo', 'portanto'}\n","    if token.lower() in cconj:\n","        return ('CCONJ', token, token, {})\n","\n","    sconj = {'que', 'quando', 'como', 'se', 'porque', 'embora', 'conquanto', 'desde', 'enquanto'}\n","    if token.lower() in sconj:\n","        return ('SCONJ', token, token, {})\n","\n","    if re.match(r'^\\d+$', token) or token.lower() in {'um', 'dois', 'três', 'quatro', 'cinco', 'seis', 'sete', 'oito', 'nove', 'dez', 'primeiro', 'segundo', 'terceiro', 'quarto', 'quinto', 'sexto', 'sétimo', 'oitavo', 'nono', 'décimo', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'}:\n","        return ('NUM', token, token, {})\n","\n","    if token.istitle():\n","        return ('PROPN', token, token, {})\n","\n","    if token.lower().endswith(('dade', 'idade', 'ção', 'mento', 'menta', 'ência', 'ância', 'ez', 'eza', 'ismo', 'ista', 'ção', 'são', 'oso', 'osa')):\n","        return ('NOUN', token, token, {})\n","\n","    verb_flexions = {\n","        'sou': ('VERB', 'ser', 'sou', {'Person': '1', 'Number': 'Sing', 'Tense': 'Pres', 'Mood': 'Ind'}),\n","        'és': ('VERB', 'ser', 'és', {'Person': '2', 'Number': 'Sing', 'Tense': 'Pres', 'Mood': 'Ind'}),\n","        'é': ('VERB', 'ser', 'é', {'Person': '3', 'Number': 'Sing', 'Tense': 'Pres', 'Mood': 'Ind'}),\n","        'somos': ('VERB', 'ser', 'somos', {'Person': '1', 'Number': 'Plur', 'Tense': 'Pres', 'Mood': 'Ind'}),\n","        'sois': ('VERB', 'ser', 'sois', {'Person': '2', 'Number': 'Plur', 'Tense': 'Pres', 'Mood': 'Ind'}),\n","        'são': ('VERB', 'ser', 'são', {'Person': '3', 'Number': 'Plur', 'Tense': 'Pres', 'Mood': 'Ind'}),\n","        'fui': ('VERB', 'ser', 'fui', {'Person': '1', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Ind'}),\n","        'foste': ('VERB', 'ser', 'foste', {'Person': '2', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Ind'}),\n","        'foi': ('VERB', 'ser', 'foi', {'Person': '3', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Ind'}),\n","        'fomos': ('VERB', 'ser', 'fomos', {'Person': '1', 'Number': 'Plur', 'Tense': 'Past', 'Mood': 'Ind'}),\n","        'fostes': ('VERB', 'ser', 'fostes', {'Person': '2', 'Number': 'Plur', 'Tense': 'Past', 'Mood': 'Ind'}),\n","        'foram': ('VERB', 'ser', 'foram', {'Person': '3', 'Number': 'Plur', 'Tense': 'Past', 'Mood': 'Ind'}),\n","        'serei': ('VERB', 'ser', 'serei', {'Person': '1', 'Number': 'Sing', 'Tense': 'Fut', 'Mood': 'Ind'}),\n","        'serás': ('VERB', 'ser', 'serás', {'Person': '2', 'Number': 'Sing', 'Tense': 'Fut', 'Mood': 'Ind'}),\n","        'será': ('VERB', 'ser', 'será', {'Person': '3', 'Number': 'Sing', 'Tense': 'Fut', 'Mood': 'Ind'}),\n","        'seremos': ('VERB', 'ser', 'seremos', {'Person': '1', 'Number': 'Plur', 'Tense': 'Fut', 'Mood': 'Ind'}),\n","        'sereis': ('VERB', 'ser', 'sereis', {'Person': '2', 'Number': 'Plur', 'Tense': 'Fut', 'Mood': 'Ind'}),\n","        'serão': ('VERB', 'ser', 'serão', {'Person': '3', 'Number': 'Plur', 'Tense': 'Fut', 'Mood': 'Ind'}),\n","        'seja': ('VERB', 'ser', 'seja', {'Person': '1', 'Number': 'Sing', 'Tense': 'Pres', 'Mood': 'Sub'}),\n","        'sejas': ('VERB', 'ser', 'sejas', {'Person': '2', 'Number': 'Sing', 'Tense': 'Pres', 'Mood': 'Sub'}),\n","        'seja': ('VERB', 'ser', 'seja', {'Person': '3', 'Number': 'Sing', 'Tense': 'Pres', 'Mood': 'Sub'}),\n","        'sejamos': ('VERB', 'ser', 'sejamos', {'Person': '1', 'Number': 'Plur', 'Tense': 'Pres', 'Mood': 'Sub'}),\n","        'sejais': ('VERB', 'ser', 'sejais', {'Person': '2', 'Number': 'Plur', 'Tense': 'Pres', 'Mood': 'Sub'}),\n","        'sejam': ('VERB', 'ser', 'sejam', {'Person': '3', 'Number': 'Plur', 'Tense': 'Pres', 'Mood': 'Sub'}),\n","        'fosse': ('VERB', 'ser', 'fosse', {'Person': '1', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Sub'}),\n","        'fosses': ('VERB', 'ser', 'fosses', {'Person': '2', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Sub'}),\n","        'fosse': ('VERB', 'ser', 'fosse', {'Person': '3', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Sub'}),\n","        'fôssemos': ('VERB', 'ser', 'fôssemos', {'Person': '1', 'Number': 'Plur', 'Tense': 'Past', 'Mood': 'Sub'}),\n","        'fossem': ('VERB', 'ser', 'fossem', {'Person': '3', 'Number': 'Plur', 'Tense': 'Past', 'Mood': 'Sub'}),\n","        'for': ('VERB', 'ser', 'for', {'Person': '1', 'Number': 'Sing', 'Tense': 'Fut', 'Mood': 'Sub'}),\n","        'fores': ('VERB', 'ser', 'fores', {'Person': '2', 'Number': 'Sing', 'Tense': 'Fut', 'Mood': 'Sub'}),\n","        'for': ('VERB', 'ser', 'for', {'Person': '3', 'Number': 'Sing', 'Tense': 'Fut', 'Mood': 'Sub'}),\n","        'formos': ('VERB', 'ser', 'formos', {'Person': '1', 'Number': 'Plur', 'Tense': 'Fut', 'Mood': 'Sub'}),\n","        'forem': ('VERB', 'ser', 'forem', {'Person': '3', 'Number': 'Plur', 'Tense': 'Fut', 'Mood': 'Sub'})\n","    }\n","    if token.lower() in verb_flexions:\n","        return verb_flexions[token.lower()]\n","\n","    if token.lower().endswith(('ar', 'er', 'ir', 'ado', 'ido', 'ando', 'endo', 'indo', 'ada')):\n","        return ('VERB', token, token, {})\n","\n","    if token.lower().endswith(('vel', 'al', 'ar', 'oso', 'osa', 'ível')):\n","        return ('ADJ', token, token, {})\n","\n","    aux_verbs = {'ser', 'estar', 'ter', 'haver'}\n","    if token.lower() in aux_verbs:\n","        return ('AUX', token, token, {})\n","\n","    interjections = {'oh', 'ah', 'eh', 'ai', 'epa', 'olá', 'oi'}\n","    if token.lower() in interjections:\n","        return ('INTJ', token, token, {})\n","\n","    particles = {'não', 'sim', 'talvez', 'quase', 'já', 'mesmo', 'ainda', 'até', 'bem', 'só', 'também', 'apenas'}\n","    if token.lower() in particles:\n","        return ('PART', token, token, {})\n","    else:\n","        return None\n","def preprocess_data(texts, pos_tags, lemmas, feats):\n","    preprocessed_texts = []\n","    preprocessed_pos_tags = []\n","    preprocessed_lemmas = []\n","    preprocessed_feats = []\n","    true_pos_tags = []\n","    true_lemmas = []\n","    true_feats = []\n","    for text, pos_tag, lemma, feat in zip(texts, pos_tags, lemmas, feats):\n","        result = preprocess_token(text)\n","        if result:\n","            preprocessed_texts.append(result[1])\n","            preprocessed_pos_tags.append(result[0])\n","            preprocessed_lemmas.append(result[2])\n","            preprocessed_feats.append(result[3])\n","            true_pos_tags.append(pos_tag)\n","            true_lemmas.append(lemma)\n","            true_feats.append(feat)\n","    return preprocessed_texts, preprocessed_pos_tags, preprocessed_lemmas, preprocessed_feats, true_pos_tags, true_lemmas, true_feats\n","\n","def unprocessed_data(texts, pos_tags, lemmas, feats):\n","    unprocessed_texts = []\n","    unprocessed_pos_tags = []\n","    unprocessed_lemmas = []\n","    unprocessed_feats = []\n","    for text, pos_tag, lemma, feat in zip(texts, pos_tags, lemmas, feats):\n","        if not preprocess_token(text):\n","            unprocessed_texts.append(text)\n","            unprocessed_pos_tags.append(pos_tag)\n","            unprocessed_lemmas.append(lemma)\n","            unprocessed_feats.append(feat)\n","    return unprocessed_texts, unprocessed_pos_tags, unprocessed_lemmas, unprocessed_feats\n"]},{"cell_type":"markdown","metadata":{"id":"AqKU9ntKGoC1"},"source":["1.2 Carregando e tratando dados de treinamento"]},{"cell_type":"markdown","metadata":{"id":"o2HSrm0SkzaS"},"source":["1.2.1 Dataset de treino"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"u9emHcuhGoC2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724003905615,"user_tz":180,"elapsed":13604,"user":{"displayName":"Marlo Souza","userId":"17413101361011853171"}},"outputId":"8c24d84c-ed22-4f4e-dfdb-2b6135e69908"},"outputs":[{"output_type":"stream","name":"stdout","text":["(82829, 4)\n"]}],"source":["# Carregar o corpus Bosque\n","bosque = CoNLLU(files=[\"pt_porttinari-ud-train.conllu\"])\n","\n","#separar features:y_pred\n","texts, pos_tags, lemmas, feats = extract_data(bosque)\n","\n","df = pd.DataFrame({'text': texts, 'pos': pos_tags, 'lemma': lemmas, 'feats': feats})\n","\n","dados_preprocessados_raw = preprocess_data(texts, pos_tags, lemmas, feats)\n","dados_nao_preprocessados = unprocessed_data(texts, pos_tags, lemmas, feats)\n","\n","dados_preprocessados = pd.DataFrame(\n","    {'text': dados_preprocessados_raw[0],\n","     'pos': dados_preprocessados_raw[1],\n","     'lemma': dados_preprocessados_raw[2],\n","     'feats': dados_preprocessados_raw[3]\n","    }\n",")\n","valores_reais = pd.DataFrame(\n","    {'text': dados_preprocessados_raw[0],\n","     'pos': dados_preprocessados_raw[4],\n","     'lemma': dados_preprocessados_raw[5],\n","     'feats': dados_preprocessados_raw[6]\n","    }\n",")\n","dados_nao_preprocessados = pd.DataFrame({\n","    'text': dados_nao_preprocessados[0],\n","    'pos': dados_nao_preprocessados[1],\n","    'lemma': dados_nao_preprocessados[2],\n","    'feats': dados_nao_preprocessados[3]\n","    }\n",")\n","print(dados_preprocessados.shape) # quantidade de tokens que foram pré-processados\n","dados_nao_preprocessados = dados_nao_preprocessados[dados_nao_preprocessados['pos'] != 'X']\n","dados_nao_preprocessados = dados_nao_preprocessados[dados_nao_preprocessados['pos'] != 'PUNCT']\n"]},{"cell_type":"markdown","metadata":{"id":"-XA5aff6GoC4"},"source":["1.2.2 Dataset de teste"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"v89u1cAiGoC4","executionInfo":{"status":"ok","timestamp":1724003912184,"user_tz":180,"elapsed":3256,"user":{"displayName":"Marlo Souza","userId":"17413101361011853171"}}},"outputs":[],"source":["test = CoNLLU(files=[\"pt_porttinari-ud-test.conllu\"])\n","texts_test, pos_tags_test, lemmas_test, feats_test = extract_data(test)\n","df_test = pd.DataFrame({'text': texts_test, 'pos': pos_tags_test, 'lemma': lemmas_test, 'feats': feats_test})\n","tamanhoTotal = len(df_test)\n","dados_preprocessados_test_raw = preprocess_data(texts_test, pos_tags_test, lemmas_test, feats_test)\n","dados_nao_preprocessados_test = unprocessed_data(texts_test, pos_tags_test, lemmas_test, feats_test)\n","dados_preprocessados_test = pd.DataFrame(\n","    {'text': dados_preprocessados_test_raw[0],\n","     'pos': dados_preprocessados_test_raw[1],\n","     'lemma': dados_preprocessados_test_raw[2],\n","     'feats': dados_preprocessados_test_raw[3]\n","    }\n",")\n","tamanhoPreProcessados = len(dados_preprocessados_test)\n","valores_reais_test = pd.DataFrame(\n","    {'text': dados_preprocessados_test_raw[0],\n","     'pos': dados_preprocessados_test_raw[4],\n","     'lemma': dados_preprocessados_test_raw[5],\n","     'feats': dados_preprocessados_test_raw[6]\n","    })\n","dados_nao_preprocessados_test = pd.DataFrame({\n","    'text': dados_nao_preprocessados_test[0],\n","    'pos': dados_nao_preprocessados_test[1],\n","    'lemma': dados_nao_preprocessados_test[2],\n","    'feats': dados_nao_preprocessados_test[3]\n","})\n","tamanhoNaoPreProcessados = len(dados_nao_preprocessados_test)\n","dados_nao_preprocessados_test = dados_nao_preprocessados_test[dados_nao_preprocessados_test['pos'] != 'X']\n","dados_nao_preprocessados_test = dados_nao_preprocessados_test[dados_nao_preprocessados_test['pos'] != 'PUNCT']"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"g-EtV2vE7I1-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e997fab-b425-4afa-96eb-238b11061334","executionInfo":{"status":"ok","timestamp":1724003916309,"user_tz":180,"elapsed":400,"user":{"displayName":"Marlo Souza","userId":"17413101361011853171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(82829, 4)\n","(34998, 4)\n","(23709, 4)\n","(9817, 4)\n"]}],"source":["print(dados_preprocessados.shape)\n","print(dados_nao_preprocessados.shape)\n","print(dados_preprocessados_test.shape)\n","print(dados_nao_preprocessados_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"zfbgFmgcGoC5"},"source":["1.3 Acurácia do Pré-Processamento"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"JEVz8Rm3GoC6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"07081b73-c638-47c4-bd12-e06d4d1dd5c9","executionInfo":{"status":"ok","timestamp":1724003934005,"user_tz":180,"elapsed":730,"user":{"displayName":"Marlo Souza","userId":"17413101361011853171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.784596566704627\n","0.797460879834662\n","0.5748028174954659\n"]}],"source":["def calcular_acuracia(valores_reais,df_preprocessado,target):\n","    acertos=0\n","    erros=0\n","    if target=='pos':\n","        y_true=valores_reais['pos']\n","        y_pred=df_preprocessado['pos']\n","        for i in range(len(y_true)):\n","            if y_true[i]!=y_pred[i]:\n","                erros+=1\n","            else:\n","                acertos+=1\n","    elif target=='lemma':\n","        y_true=valores_reais['lemma']\n","        y_pred=df_preprocessado['lemma']\n","        for i in range(len(y_true)):\n","            if y_true[i]!=y_pred[i]:\n","                erros+=1\n","            else:\n","                acertos+=1\n","    elif target=='feats':\n","        y_true=valores_reais['feats']\n","        y_pred=df_preprocessado['feats']\n","        for i in range(len(y_true)):\n","            if y_true[i]!=y_pred[i]:\n","                erros+=1\n","            else:\n","                acertos+=1\n","    return acertos/(acertos+erros)\n","acuracia_pos_pre=calcular_acuracia(valores_reais_test,dados_preprocessados_test,'pos')\n","acuracia_lemma_pre=calcular_acuracia(valores_reais_test,dados_preprocessados_test,'lemma')\n","acuracia_feats_pre=calcular_acuracia(valores_reais_test,dados_preprocessados_test,'feats')\n","print(acuracia_pos_pre)\n","print(acuracia_lemma_pre)\n","print(acuracia_feats_pre) ## acurácia do modelo pré-processado pra feats é muito baixa,portanto aplicar aprendizado de máquina somente para este caso"]},{"cell_type":"markdown","metadata":{"id":"-v3rxjHilHdb"},"source":["##Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"71kGopvrRfWx"},"source":["#POS-TAGGING:Análise de classes"]},{"cell_type":"markdown","metadata":{"id":"vjHCSkSlGoC7"},"source":["Treinamento do modelo usando os dados não pré-processados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TV3wG1ngEc3F"},"outputs":[],"source":["#Separando dados para treinamento do POS-TAGGING\n","X_train = dados_nao_preprocessados['text']\n","y_train = dados_nao_preprocessados['pos']\n","X_test = dados_nao_preprocessados_test['text']\n","y_test = dados_nao_preprocessados_test['pos']\n","# Definir o pipeline\n","pipeline_pos = Pipeline([\n","    ('vectorizer', CountVectorizer()),\n","    ('classifier', DecisionTreeClassifier(criterion='gini', min_samples_split=10))\n","])\n","# Treinar o modelo\n","pipeline_pos.fit(X_train, y_train)\n","# Fazer predições\n","y_pred = pipeline_pos.predict(X_test)\n","#salvando predicoes num dataframe\n","unprocess_pred = pd.DataFrame({'text': X_test, 'pos': y_pred})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcrfFOW_AlIi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef33d403-2d29-456c-a11b-c808dc15d729"},"outputs":[{"output_type":"stream","name":"stdout","text":["          text   pos\n","0         você  PRON\n","1      recebeu  VERB\n","2      notícia  NOUN\n","3        seria   AUX\n","4  substituído  VERB\n"]}],"source":["print(unprocess_pred.head())"]},{"cell_type":"markdown","metadata":{"id":"yKS5_oSkGoC9"},"source":["Avaliando modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9BYP6iTGoC9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14702b94-e35c-46ad-b612-2d97c05afefe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Acurácia no conjunto de treinamento: 0.9463186356073211\n","Acurácia no conjunto de teste: 0.7837383660806618\n"]}],"source":["# Avaliar o modelo no conjunto de treinamento\n","train_accuracy = pipeline_pos.score(X_train, y_train)\n","#Medindo a acurácia no conjunto de teste\n","test_accuracy = pipeline_pos.score(X_test, y_test)\n","print('Acurácia no conjunto de treinamento:', train_accuracy)\n","print('Acurácia no conjunto de teste:', test_accuracy)\n","acuracia_pos=test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"59GAONRZRpVo"},"source":["#LEMMATIZACAO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKvicqqeGoC_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f65d12a6-fa97-40fc-887e-2be0bbc84e8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["48162    concentrados\n","48163      campeonato\n","48164            onde\n","48165          ocupam\n","48166        invictos\n","Name: text, dtype: object\n","48162    concentrado\n","48163     campeonato\n","48164           onde\n","48165         ocupar\n","48166        invicto\n","Name: lemma, dtype: object\n","(48080,)\n","(48080,)\n"]}],"source":["x_train = pd.Series(dados_nao_preprocessados['text'])\n","y_train = pd.Series(dados_nao_preprocessados['lemma'])\n","x_test = pd.Series(dados_nao_preprocessados_test['text'])\n","y_test = pd.Series(dados_nao_preprocessados_test['lemma'])\n","print(X_train.tail())\n","print(y_train.tail())\n","print(X_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ApcPIRtGoDA"},"outputs":[],"source":["\n","# Definir o pipeline\n","pipeline_lemma = Pipeline([\n","    ('vectorizer', CountVectorizer()),\n","    ('classifier', DecisionTreeClassifier(criterion='gini', min_samples_split=10))\n","])\n","# Treinar o modelo\n","pipeline_lemma.fit(X_train, y_train)\n","# Fazer predições\n","y_pred = pipeline_lemma.predict(X_test)\n","#salvando predicoes num dataframe\n","unprocess_pred = unprocess_pred.join(pd.DataFrame({'lemma': y_pred}))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q392rPvLGoDA","outputId":"3a51897f-7700-41f2-a5b5-34b7961f1c9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Acurácia no conjunto de treinamento: 0.972608153078203\n","Acurácia no conjunto de teste: 0.8085573940020683\n"]}],"source":["train_accuracy = pipeline_lemma.score(X_train, y_train)\n","print('Acurácia no conjunto de treinamento:', train_accuracy)\n","#Medindo a acurácia no conjunto de teste\n","test_accuracy = pipeline_lemma.score(X_test, y_test)\n","print('Acurácia no conjunto de teste:', test_accuracy)\n","acuracia_lemma=test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"e0_B4aAiGoDA"},"source":["#Analisador de flexão"]},{"cell_type":"markdown","metadata":{"id":"GiEifkbNGoDC"},"source":["Normalizando feats e analisando dados\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4E2nXZulGoDC"},"outputs":[],"source":["X_train = df['text']\n","y_train_raw = df['feats']\n","X_test = df_test['text']\n","y_test_raw = df_test['feats']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3Ez6cLHNAfR","outputId":"7604e906-77a1-437e-81cd-3175e61ba4b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["0          PT\n","1          em\n","2           o\n","3     governo\n","4    BRASÍLIA\n","Name: text, dtype: object\n"]}],"source":["print(X_train.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPchFTXkGoDD","outputId":"0c55db3e-664e-4797-faf0-b10e6e454797"},"outputs":[{"output_type":"stream","name":"stdout","text":["(171776,)\n","(171776,)\n"]}],"source":["y_train_raw = pd.Series(y_train_raw)\n","y_test_raw = pd.Series(y_test_raw)\n","print(X_train.shape)\n","print(y_train_raw.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FUDvemf4y6F","outputId":"1ea7c95c-e319-4528-bdb1-fe3e6d707ec4"},"outputs":[{"output_type":"stream","name":"stdout","text":["0                 {'Gender': 'Masc', 'Number': 'Sing'}\n","1                                                   {}\n","2    {'Definite': 'Def', 'Gender': 'Masc', 'Number'...\n","3                 {'Gender': 'Masc', 'Number': 'Sing'}\n","4                  {'Gender': 'Fem', 'Number': 'Sing'}\n","Name: feats, dtype: object\n"]}],"source":["print(y_train_raw.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjB_0BhKGoDE","outputId":"a6080f86-6590-4c0f-e91e-5be0406f51e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["   Abbr  Case Definite ExtPos Gender  Mood NumType Number Person Polarity  \\\n","0  None  None     None   None   Masc  None    None   Sing   None     None   \n","1  None  None     None   None   None  None    None   None   None     None   \n","2  None  None      Def   None   Masc  None    None   Sing   None     None   \n","3  None  None     None   None   Masc  None    None   Sing   None     None   \n","4  None  None     None   None    Fem  None    None   Sing   None     None   \n","\n","  PronType Tense  Typo VerbForm Voice  \n","0     None  None  None     None  None  \n","1     None  None  None     None  None  \n","2      Art  None  None     None  None  \n","3     None  None  None     None  None  \n","4     None  None  None     None  None  \n"]}],"source":["y_train = pd.json_normalize(y_train_raw)\n","y_test = pd.json_normalize(y_test_raw)\n","y_train = y_train.fillna('None')\n","y_test = y_test.fillna('None')\n","common_columns = set(y_train.columns).intersection(set(y_test.columns))\n","y_train = y_train[sorted(common_columns)]\n","y_test = y_test[sorted(common_columns)]\n","print(y_train.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy-btLEKftMI"},"outputs":[],"source":["#Selecionar colunas desejadas\n","y_train = y_train[['Gender', 'Number', 'Person', 'Tense', 'VerbForm', 'PronType']]\n","y_test = y_test[['Gender', 'Number', 'Person', 'Tense', 'VerbForm', 'PronType']]"]},{"cell_type":"markdown","metadata":{"id":"1QOPjIfiGoDE"},"source":["Conclusões da análise do dataset:\n","* target possuem muitas colunas de dados do tipo categórico que seguem regras,logo seria interessente dividir o treinamento em c colunas,aplicar one hot encoder e treinar usando árvores para cada coluna e ao fim juntar todas as colunas previstas."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ByFumLdGoDF","outputId":"27d3b8ac-f4b3-4720-8441-d0739a3c7544"},"outputs":[{"output_type":"stream","name":"stdout","text":["Acurácia no conjunto de treinamento: 0.8309542660208644\n","Acurácia no conjunto de teste: 0.7729676858426315\n","Acurácia no conjunto de treinamento: 0.8539260432190761\n","Acurácia no conjunto de teste: 0.7769526155629619\n","Acurácia no conjunto de treinamento: 0.986395072652757\n","Acurácia no conjunto de teste: 0.9739168236487465\n","Acurácia no conjunto de treinamento: 0.9902023565573771\n","Acurácia no conjunto de teste: 0.9794594986233879\n","Acurácia no conjunto de treinamento: 0.9884093237704918\n","Acurácia no conjunto de teste: 0.9650775249963773\n","Acurácia no conjunto de treinamento: 0.8853914400149031\n","Acurácia no conjunto de teste: 0.8857411969279815\n"]}],"source":["\n","#transformando as features em vetores\n","#onde armazenaremos a acurácia de cada modelo e os modelos treinados\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)\n","modelos = []\n","ys_pred_test = []\n","encoders = []\n","#aplicando one hot encoder(dados categoricos) e treinando os modelos\n","for col in range(y_train.shape[1]):\n","    #aplicando one hot encoder\n","    enc = OneHotEncoder(handle_unknown='ignore')\n","    y_train_enc = enc.fit_transform(y_train[:, col].reshape(-1, 1)).toarray()\n","    y_test_enc = enc.transform(y_test[:, col].reshape(-1, 1)).toarray()\n","    #treinando o modelo\n","    pipeline_feats = Pipeline([\n","        ('vectorizer', CountVectorizer()),\n","        ('classifier', DecisionTreeClassifier(criterion='gini', min_samples_split=10))\n","    ])\n","    pipeline_feats.fit(X_train, y_train_enc)\n","    modelos.append(pipeline_feats)\n","    #medindo a acurácia\n","    y_pred_train = pipeline_feats.predict(X_train)\n","    train_accuracy = pipeline_feats.score(X_train, y_train_enc)\n","    print('Acurácia no conjunto de treinamento:', train_accuracy)\n","    y_pred_test = pipeline_feats.predict(X_test)\n","    test_accuracy = pipeline_feats.score(X_test, y_test_enc)\n","    print('Acurácia no conjunto de teste:', test_accuracy)\n","    #convertendo as previões para o formato original\n","    y_pred_test = enc.inverse_transform(y_pred_test)\n","    ys_pred_test.append(y_pred_test)\n","    encoders.append(enc)\n"]},{"cell_type":"markdown","metadata":{"id":"5nVZDgYFGoDF"},"source":["Calculo da acurácia do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuslCWT4GoDF","outputId":"031949af-09e9-4919-f7d4-16a275369871"},"outputs":[{"output_type":"stream","name":"stdout","text":["  feats_0 feats_1 feats_2 feats_3 feats_4 feats_5\n","0     Fem    Sing    None    None    None    None\n","1    None    None    None    None    None    None\n","2    None    None    None    None    None    None\n","3    None    Sing       3    None    None     Prs\n","4    None    Sing       3    Past     Fin    None\n","  feats_0 feats_1 feats_2 feats_3 feats_4 feats_5\n","0     Fem    Sing    None    None    None    None\n","1    None    None    None    None    None    None\n","2    None    None    None    None    None    None\n","3    None    Sing       3    None    None     Prs\n","4    None    Sing       3    Past     Fin    None\n","Acurácia no conjunto de teste: 0.73\n"]}],"source":["#Realizando a predição no conjunto de teste\n","for i in range(len(ys_pred_test)):\n","    ys_pred_test[i] = pd.DataFrame(ys_pred_test[i], columns=[f'feats_{i}'])\n","y_pred_test = pd.concat(ys_pred_test, axis=1)\n","y_pred_test = y_pred_test.fillna('None')\n","print(y_pred_test.head())\n","y_test = pd.DataFrame(y_test, columns=[f'feats_{i}' for i in range(y_test.shape[1])])\n","print(y_test.head())\n","#Calculando a acurácia\n","acertos, erros = 0, 0\n","for i in range(y_test.shape[0]):\n","    if y_test.iloc[i].equals(y_pred_test.iloc[i]):\n","        acertos += 1\n","    else:\n","        erros += 1\n","accuracy = acertos/(acertos+erros)\n","print(f'Acurácia no conjunto de teste: {accuracy:.2f}')\n","acuracia_feats=accuracy\n"]},{"cell_type":"markdown","metadata":{"id":"C0obQ7NeGoDG"},"source":["Organizando os datasets de teste e previsão para calcular a acurácia\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVTENsU_GoDG","outputId":"9df48dfe-5ad7-42d1-8eba-820677e6f997"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Gender Number Person Tense VerbForm PronType\n","0    Fem   Sing   None  None     None     None\n","1   None   None   None  None     None     None\n","2   None   None   None  None     None     None\n","3   None   Sing      3  None     None      Prs\n","4   None   Sing      3  Past      Fin     None\n","  Gender Number Person Tense VerbForm PronType\n","0    Fem   Sing   None  None     None     None\n","1   None   None   None  None     None     None\n","2   None   None   None  None     None     None\n","3   None   Sing      3  None     None      Prs\n","4   None   Sing      3  Past      Fin     None\n"]}],"source":["y_pred_test.columns = ['Gender', 'Number', 'Person', 'Tense', 'VerbForm', 'PronType']\n","y_test.columns = ['Gender', 'Number', 'Person', 'Tense', 'VerbForm', 'PronType']\n","print(y_pred_test.head())\n","print(y_test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zea9cjh-IHUE"},"outputs":[],"source":["df_test = df_test.join(y_test)\n","df_test = df_test.drop(columns=['feats'])\n","df_test = df_test.fillna('None')"]},{"cell_type":"code","source":["print(df_test.head())"],"metadata":{"id":"YznORuFC7NaX","outputId":"4f5d9ebc-b007-463f-fa51-50f19c58aba8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      text    pos    lemma Gender Number Person Tense VerbForm PronType\n","0    Folha  PROPN    Folha    Fem   Sing   None  None     None     None\n","1       --  PUNCT       --   None   None   None  None     None     None\n","2     Como    ADV     como   None   None   None  None     None     None\n","3     você   PRON     você   None   Sing      3  None     None      Prs\n","4  recebeu   VERB  receber   None   Sing      3  Past      Fin     None\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bpd8YAFCTTxq","outputId":"53a45ffe-ed21-40e2-e230-bf1707688ab9"},"outputs":[{"output_type":"stream","name":"stdout","text":["      text   pos lemma Gender Number Person Tense VerbForm PronType\n","0    Folha  None  None    Fem   Sing   None  None     None     None\n","1       --  None  None   None   None   None  None     None     None\n","2     Como  None  None   None   None   None  None     None     None\n","3     você  None  None   None   Sing      3  None     None      Prs\n","4  recebeu  None  None   None   Sing      3  Past      Fin     None\n"]}],"source":["previsoes = pd.DataFrame({'text': X_test, 'pos': None, 'lemma': None})\n","previsoes = previsoes.join(y_pred_test)\n","print(previsoes.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_FwSOujT_Bc","outputId":"561a3a37-47ba-4553-e51f-2407488e199b"},"outputs":[{"output_type":"stream","name":"stdout","text":["      text    pos  lemma Gender Number Person Tense VerbForm PronType\n","0    Folha  PROPN  Folha    Fem   Sing   None  None     None     None\n","1       --    SYM     --   None   None   None  None     None     None\n","2     Como  SCONJ   Como   None   None   None  None     None     None\n","3     você    ADP      a   None   Sing      3  None     None      Prs\n","4  recebeu    ADP     de   None   Sing      3  Past      Fin     None\n","      text    pos    lemma Gender Number Person Tense VerbForm PronType\n","0    Folha  PROPN    Folha    Fem   Sing   None  None     None     None\n","1       --  PUNCT       --   None   None   None  None     None     None\n","2     Como    ADV     como   None   None   None  None     None     None\n","3     você   PRON     você   None   Sing      3  None     None      Prs\n","4  recebeu   VERB  receber   None   Sing      3  Past      Fin     None\n"]}],"source":["for i in range(df_test.shape[1]):\n","    if dados_nao_preprocessados_test['text'][i]==previsoes['text'][i]:\n","        previsoes['pos'][i] = unprocess_pred['pos'][i]\n","        previsoes['lemma'][i] = unprocess_pred['lemma'][i]\n","    else:\n","        previsoes['pos'][i] = dados_preprocessados_test['pos'][i]\n","        previsoes['lemma'][i] = dados_preprocessados_test['lemma'][i]\n","print(previsoes.head())\n","print(df_test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYaxqsdfc9hp","outputId":"7356c3c9-7778-48af-e55a-393e9a2d2974"},"outputs":[{"output_type":"stream","name":"stdout","text":["(27604, 9)\n","(27604, 9)\n"]}],"source":["print(previsoes.shape)\n","print(df_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKIbaGgccdUn","outputId":"bf0d1dde-1e74-494f-c2ad-13a9d6fe8710"},"outputs":[{"output_type":"stream","name":"stdout","text":["      text    pos    lemma Gender Number Person Tense VerbForm PronType\n","0    Folha  PROPN    Folha    Fem   Sing   None  None     None     None\n","1       --  PUNCT       --   None   None   None  None     None     None\n","2     Como    ADV     como   None   None   None  None     None     None\n","3     você   PRON     você   None   Sing      3  None     None      Prs\n","4  recebeu   VERB  receber   None   Sing      3  Past      Fin     None\n","      text    pos  lemma Gender Number Person Tense VerbForm PronType\n","0    Folha  PROPN  Folha    Fem   Sing   None  None     None     None\n","1       --    SYM     --   None   None   None  None     None     None\n","2     Como  SCONJ   Como   None   None   None  None     None     None\n","3     você    ADP      a   None   Sing      3  None     None      Prs\n","4  recebeu    ADP     de   None   Sing      3  Past      Fin     None\n"]}],"source":["#feats que serão consideradas:Gender,Number,Person,Tense,VerbForm,PronType\n","# Seleção das colunas específicas\n","df_test_selected = df_test[['text', 'pos', 'lemma', 'Gender', 'Number', 'Person', 'Tense', 'VerbForm', 'PronType']]\n","previsoes_selected = previsoes[['text', 'pos', 'lemma', 'Gender', 'Number', 'Person', 'Tense', 'VerbForm', 'PronType']]\n","\n","# Impressão das primeiras linhas dos DataFrames selecionados\n","print(df_test_selected.head())\n","print(previsoes_selected.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUOBGYexbSyU"},"outputs":[],"source":["previsoes_ord = previsoes.sort_values(by='text')\n","previsoes_ord = previsoes_ord.reset_index(drop=True)\n","previsoes_ord = previsoes_ord[df_test.columns]"]},{"cell_type":"markdown","metadata":{"id":"cvZfbdpjGoDH"},"source":["#Calculando a acurácia total"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_PKNXy6GoDI","outputId":"7b6f558e-24e0-449e-e92e-d9b4d6e2832b"},"outputs":[{"output_type":"stream","name":"stdout","text":["A acurácia obtida foi de 67.26968917548182%\n"]}],"source":["def calcular_acuracia_total(df_test, previsoes):\n","    acertos = 0\n","    total_atributos = 8  # Total de atributos a serem comparados\n","\n","    for i in range(len(df_test)):\n","        acertos_item = 0\n","\n","        if df_test['pos'][i] == previsoes['pos'][i]:\n","            acertos_item += 1\n","\n","        if df_test['lemma'][i] == previsoes['lemma'][i]:\n","            acertos_item += 1\n","\n","        if df_test['Gender'][i] == previsoes['Gender'][i]:\n","            acertos_item += 1\n","\n","        if df_test['Number'][i] == previsoes['Number'][i]:\n","            acertos_item += 1\n","\n","        if df_test['Person'][i] == previsoes['Person'][i]:\n","            acertos_item += 1\n","\n","        if df_test['Tense'][i] == previsoes['Tense'][i]:\n","            acertos_item += 1\n","\n","        if df_test['VerbForm'][i] == previsoes['VerbForm'][i]:\n","            acertos_item += 1\n","\n","        if df_test['PronType'][i] == previsoes['PronType'][i]:\n","            acertos_item += 1\n","\n","        acertos += acertos_item\n","\n","    total_comparacoes = len(df_test) * total_atributos\n","    acuracia = acertos / total_comparacoes\n","    return acuracia\n","print(f'A acurácia obtida foi de {calcular_acuracia_total(df_test_selected, previsoes_selected)*100}%')"]},{"cell_type":"markdown","metadata":{"id":"8yE4DknRzXK8"},"source":["#Criando o Analisador Morfológico"]},{"cell_type":"code","source":["import pandas as pd\n","\n","def AnalisadorMorfológico(texto):\n","    lemmas = []\n","    pos_tags = []\n","    feats = []\n","\n","    feats_template = {\n","        'Gender': None,\n","        'Number': None,\n","        'Person': None,\n","        'Tense': None,\n","        'VerbForm': None,\n","        'PronType': None\n","    }\n","\n","    # Lista das chaves de características na ordem dos modelos\n","    chaves_caracteristicas = list(feats_template.keys())\n","\n","    # Tokenizando a string de entrada\n","    if isinstance(texto, str):\n","        texto = texto.split()\n","\n","    for token in texto:\n","        result = preprocess_token(token)\n","        if result:\n","            pos_tags.append(result[0])\n","            lemmas.append(result[2])\n","        else:\n","            lemmas.append(pipeline_lemma.predict([token])[0])\n","            pos_tags.append(pipeline_pos.predict([token])[0])\n","\n","        token_feats = {}\n","        for i, model in enumerate(modelos):\n","            chave = chaves_caracteristicas[i]\n","            predicao_onehot = model.predict([token])\n","            predicao_label = encoders[i].inverse_transform(predicao_onehot)[0]\n","            if predicao_label is not None:\n","                token_feats[chave] = predicao_label\n","\n","        feats.append(token_feats)\n","\n","    # Criando um DataFrame para facilitar a visualização\n","    output_df = pd.DataFrame({\n","        'Token': texto,\n","        'Lemma': lemmas,\n","        'POS': pos_tags,\n","    })\n","\n","    feats_df = pd.DataFrame(feats)\n","    output_df = pd.concat([output_df, feats_df], axis=1)\n","\n","    return output_df\n","\n","# Exemplo de uso\n","analisador = AnalisadorMorfológico('estou estudando processamento de linguagem natural')\n","print(analisador)\n"],"metadata":{"id":"SUcQzr6E9h2R","outputId":"35c3a6c2-eb14-4fa3-c763-607cb2e88a0b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["           Token          Lemma   POS  Gender  Number  Person   Tense  \\\n","0          estou          estar   AUX  [None]  [Sing]     [1]  [Pres]   \n","1      estudando      estudando  VERB  [None]  [None]  [None]  [None]   \n","2  processamento  processamento  NOUN  [Masc]  [Sing]  [None]  [None]   \n","3             de             de   ADP  [None]  [None]  [None]  [None]   \n","4      linguagem      linguagem  NOUN   [Fem]  [Sing]  [None]  [None]   \n","5        natural        natural   ADJ  [None]  [Sing]  [None]  [None]   \n","\n","  VerbForm PronType  \n","0    [Fin]   [None]  \n","1   [None]   [None]  \n","2   [None]   [None]  \n","3   [None]   [None]  \n","4   [None]   [None]  \n","5   [None]   [None]  \n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}